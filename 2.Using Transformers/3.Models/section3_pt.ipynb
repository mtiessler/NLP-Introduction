{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets evaluate transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating a Transformer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# Building the config\n",
    "config = BertConfig()\n",
    "\n",
    "# Building the model from the config\n",
    "model = BertModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  [...]\n",
       "  \"hidden_size\": 768,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  [...]\n",
       "}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The config is a blueprint of the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Different loading methods"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "config = BertConfig()\n",
    "model = BertModel(config)\n",
    "\n",
    "# Model is randomly initialized!"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This model can be used in **this state** but the output will be **trash**\n",
    "\n",
    "We must train it **first**. \n",
    "- This requires lot of data and it has an environmental impact\n",
    "\n",
    "We should reuse and share already trained models.\n",
    "\n",
    "We use the `from_pretrained()`method"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We could replace `BertModel` with the equivalent `AutoModel` class.\n",
    "In fact, is better as is checkpoint-agnostic code.\n",
    "\n",
    "In the code sample above we didnâ€™t use `BertConfig`, and instead loaded a pretrained model via the bert-base-cased identifier\n",
    "\n",
    "This is a model checkpoint that was trained by the authors of BERT themselves.\n",
    "\n",
    "The weights have been downloaded and cached (so future calls to the from_pretrained() method wonâ€™t re-download them) in the cache folder, which defaults to:\n",
    " - `~/.cache/huggingface/transformers`. \n",
    "\n",
    "You can customize your cache folder by setting the `HF_HOME` environment variable."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Saving methods"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We use the `save_pretrained()` method"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"directory_on_my_computer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This saves two files to your disk:\n",
    "- `config.json`: Attributes necessary to build the model architecture. Also contains metadata such as where the checkpoint originated and what ðŸ¤— Transformers version you were using when you last saved the checkpoint\n",
    "- `pytorch_model.bin`: State dictionary; it contains all your modelâ€™s weights. The two files go hand in hand; the configuration is necessary to know your modelâ€™s architecture, while the model weights are your modelâ€™s parameters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using a Transformer model for inference"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We already know how to load a model (`from_pretrained()`) and how to save a model (`save_pretrained()`). \n",
    "\n",
    "We can now use it for making predictions. \n",
    "\n",
    "Transformers can **only** process numbers (that the **tokenizer** generates). "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]"
   ],
   "metadata": {},
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "encoded_sequences = [\n",
    "    [101, 7592, 999, 102],\n",
    "    [101, 4658, 1012, 102],\n",
    "    [101, 3835, 999, 102],\n",
    "]"
   ],
   "metadata": {},
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "To convert the array into a **tensor**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model_inputs = torch.tensor(encoded_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using the tensor as input to the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(model_inputs)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Models (PyTorch)",
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
