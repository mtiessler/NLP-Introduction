# Introduction

Transformer models are **very large**. They have billions of parameters, and huge datasets and trainings. 

Trying all of them and deploying them is **hard**.

## The ðŸ¤— Transformer Library
The ðŸ¤— Transformer Library was created to solve this problem. 

It provides a **single API** where any model can be: 

- Loaded
- Trained
- Saved

### Features

1. **Ease of use**: Given an NLP model, in two lines of code one can
   1. Download
   2. Load
   3. Use
2. **Flexibility**: **All** models are: 
   - Pytorch `nn.Module` class
   - TensorFlow `tf.keras.Model` class
3. **Simplicity**: **All in one file** concept. A model forward pass is defined in a **single file**.
   - This last feature makes ðŸ¤— Transformers quite different from other ML libraries. 
   - The models are not built on modules that are shared across files:
     - Each model has its own layers. This makes them more approachable and understandable and allows you to easily experiment on one model without affecting others.
