{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the data (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (2.16.1)\n",
      "Requirement already satisfied: evaluate in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (0.4.1)\n",
      "Requirement already satisfied: transformers[sentencepiece] in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (4.36.2)\n",
      "Requirement already satisfied: filelock in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (from datasets) (1.26.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (from datasets) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (from datasets) (0.20.1)\n",
      "Requirement already satisfied: packaging in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: responses<0.19 in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (from transformers[sentencepiece]) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.4.1)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.1.99)\n",
      "Requirement already satisfied: protobuf in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (from transformers[sentencepiece]) (4.25.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets evaluate transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Loading a dataset from the HUB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The datasets can be found [here](https://huggingface.co/datasets)\n",
    "\n",
    "We focus in the **MRPC** dataset. One of the 10 datasets in the [GLUE benchmark](https://gluebenchmark.com/) which is an academic benchmark that is used to measure the performance of ML models across 10 different text classification tasks.\n",
    "\n",
    "We download the datasets thanks to the ðŸ¤— Datasets library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/max/PycharmProjects/NLP-Introduction/venv/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Same as before\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]\n",
    "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# This is new\n",
    "batch[\"labels\"] = torch.tensor([1, 1])\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "loss = model(**batch).loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We get a `DatasetDict` object with: \n",
    "- Training set\n",
    "- Validation set\n",
    "- Test set\n",
    "\n",
    "Each of those contain the following columns: \n",
    "- sentence1\n",
    "- sentence2\n",
    "- label\n",
    "- idx\n",
    "\n",
    "And a **variable number** of rows\n",
    "\n",
    "The dataset is downloaded and cached , by default in `~/.cache/huggingface/datasets`. Use `HF_HOME` if you want to change the location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access each pair of sentences in our `raw_datasets` object by indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know which integer corresponds to which label, we can inspect the **features** of our `raw_train_dataset` by accessing `raw_train_dataset.features`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at element 15 of the training set and element 87 of the validation set. What are their labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Training Dataset element 15: {'sentence1': 'Gyorgy Heizler , head of the local disaster unit , said the coach was carrying 38 passengers .', 'sentence2': 'The head of the local disaster unit , Gyorgy Heizler , said the coach driver had failed to heed red stop lights .', 'label': 0, 'idx': 15}\n",
      "Raw Training Dataset element 15 labels: {'sentence1': Value(dtype='string', id=None), 'sentence2': Value(dtype='string', id=None), 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None), 'idx': Value(dtype='int32', id=None)}\n",
      "Raw Validation Dataset element 87: {'sentence1': 'He was arrested Friday night at an Alpharetta seafood restaurant while dining with his wife , singer Whitney Houston .', 'sentence2': 'He was arrested again Friday night at an Alpharetta restaurant where he was having dinner with his wife .', 'label': 1, 'idx': 796}\n",
      "Raw Validation Dataset element 87 labels: {'sentence1': Value(dtype='string', id=None), 'sentence2': Value(dtype='string', id=None), 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None), 'idx': Value(dtype='int32', id=None)}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_dataset = load_dataset(\"glue\",\"mrpc\")\n",
    "raw_training_dataset = raw_dataset[\"train\"]\n",
    "print(f\"Raw Training Dataset element 15: {raw_training_dataset[14]}\")\n",
    "print(f\"Raw Training Dataset element 15 labels: {raw_training_dataset.features}\")\n",
    "raw_validation_dataset = raw_dataset[\"validation\"]\n",
    "print(f\"Raw Validation Dataset element 87: {raw_validation_dataset[86]}\")\n",
    "print(f\"Raw Validation Dataset element 87 labels: {raw_validation_dataset.features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To preprocess the dataset: \n",
    "- Use Tokenizer to convert text into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\n",
    "tokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to pass the sentences as **pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'first',\n",
       " 'sentence',\n",
       " '.',\n",
       " '[SEP]',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'second',\n",
       " 'one',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you load another **checkpoint** you will not necessarily see the `token_type_ids`.\n",
    "\n",
    "They are only returned when the model will know what to do with them, because it has seen them during its pretraining.\n",
    "\n",
    "`BERT` is pretrained with token type IDs, it has an objective called *next sentence prediction*. The goal with this task is to model the relationship between pairs of sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add padding and truncation options for pretraining.\n",
    "\n",
    "It works well but it has a disadvantage:\n",
    "- Returns a dictionary.\n",
    "\n",
    "This means that it will **only work** if you have enough RAM to store **the whole dataset during tokenization**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenizer(\n",
    "    raw_datasets[\"train\"][\"sentence1\"],\n",
    "    raw_datasets[\"train\"][\"sentence2\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep the data as a **dataset**, we will use the `.map()` function. \n",
    "\n",
    "It works by applying a function on each element of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous function, takes a dictionary (like the one in our dataset) and returns the resulting dictionary from calling the `tokenizer()` function.\n",
    "\n",
    "The `batched` option will greatly speed up the tokenization (if we give a lots of inputs).\n",
    "\n",
    "**Important**: we are leaving the `padding` option: \n",
    "- Padding all the samples to the maximum length is not efficient.\n",
    "\n",
    "Is much better: \n",
    "- Pad the samples when we are building a batch. We only pad within that batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a983467c584628ad820c71923888ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use multiprocessing in `map()` by passing a `num_proc` argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take element 15 of the training set and tokenize the two sentences separately and as a pair. Whatâ€™s the difference between the two results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "        num_rows: 3668\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "        num_rows: 408\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "        num_rows: 1725\n",
      "    })\n",
      "})\n",
      "\n",
      "\n",
      "Training Dataset: {'sentence1': 'Gyorgy Heizler , head of the local disaster unit , said the coach was carrying 38 passengers .', 'sentence2': 'The head of the local disaster unit , Gyorgy Heizler , said the coach driver had failed to heed red stop lights .', 'label': 0, 'idx': 15}\n",
      "\n",
      "Tokenized sentence 1: {'input_ids': [101, 1043, 7677, 22637, 2002, 10993, 3917, 1010, 2132, 1997, 1996, 2334, 7071, 3131, 1010, 2056, 1996, 2873, 2001, 4755, 4229, 5467, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "Tokenized sentence 2: {'input_ids': [101, 1996, 2132, 1997, 1996, 2334, 7071, 3131, 1010, 1043, 7677, 22637, 2002, 10993, 3917, 1010, 2056, 1996, 2873, 4062, 2018, 3478, 2000, 18235, 2094, 2417, 2644, 4597, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "Tokenized sentence 1 & 2: {'input_ids': [101, 1043, 7677, 22637, 2002, 10993, 3917, 1010, 2132, 1997, 1996, 2334, 7071, 3131, 1010, 2056, 1996, 2873, 2001, 4755, 4229, 5467, 1012, 102, 1996, 2132, 1997, 1996, 2334, 7071, 3131, 1010, 1043, 7677, 22637, 2002, 10993, 3917, 1010, 2056, 1996, 2873, 4062, 2018, 3478, 2000, 18235, 2094, 2417, 2644, 4597, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'g',\n",
       " '##yo',\n",
       " '##rgy',\n",
       " 'he',\n",
       " '##iz',\n",
       " '##ler',\n",
       " ',',\n",
       " 'head',\n",
       " 'of',\n",
       " 'the',\n",
       " 'local',\n",
       " 'disaster',\n",
       " 'unit',\n",
       " ',',\n",
       " 'said',\n",
       " 'the',\n",
       " 'coach',\n",
       " 'was',\n",
       " 'carrying',\n",
       " '38',\n",
       " 'passengers',\n",
       " '.',\n",
       " '[SEP]',\n",
       " 'the',\n",
       " 'head',\n",
       " 'of',\n",
       " 'the',\n",
       " 'local',\n",
       " 'disaster',\n",
       " 'unit',\n",
       " ',',\n",
       " 'g',\n",
       " '##yo',\n",
       " '##rgy',\n",
       " 'he',\n",
       " '##iz',\n",
       " '##ler',\n",
       " ',',\n",
       " 'said',\n",
       " 'the',\n",
       " 'coach',\n",
       " 'driver',\n",
       " 'had',\n",
       " 'failed',\n",
       " 'to',\n",
       " 'hee',\n",
       " '##d',\n",
       " 'red',\n",
       " 'stop',\n",
       " 'lights',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "raw_dataset = load_dataset(\"glue\",\"mrpc\")\n",
    "print(raw_dataset)\n",
    "print(\"\\n\")\n",
    "raw_training_dataset = raw_dataset[\"train\"]\n",
    "print(f\"Training Dataset: {raw_training_dataset[14]}\\n\")\n",
    "tokenized_sentence_1 = tokenizer(raw_training_dataset[14][\"sentence1\"])\n",
    "print(f\"Tokenized sentence 1: {tokenized_sentence_1}\\n\")\n",
    "tokenized_sentence_2 = tokenizer(raw_training_dataset[14][\"sentence2\"])\n",
    "print(f\"Tokenized sentence 2: {tokenized_sentence_2}\\n\")\n",
    "tokenized_sentences_pair = tokenizer(raw_training_dataset[14][\"sentence1\"], raw_training_dataset[14][\"sentence2\"])\n",
    "print(f\"Tokenized sentence 1 & 2: {tokenized_sentences_pair}\\n\")\n",
    "\n",
    "tokenizer.convert_ids_to_tokens(tokenized_sentences_pair[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference is in the: \n",
    "- token_type_id\n",
    "\n",
    "From `[CLS]` to first `[SEP]` `token_type_id = 0`\n",
    "\n",
    "Until the last `[SEP]` `token_type_id = 1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function responsible for putting together samples inside a batch:\n",
    "- `collate function`\n",
    "\n",
    "You can pass it as an **argument** when you build a `DataLoader`.\n",
    "\n",
    "If you use the default: \n",
    "- Will convert your samples to PyTorch tensors and concatenate them. (Not possible since all inputs won't have same size!)\n",
    "\n",
    "We postpone as much as we can the **padding** as we want to apply it as necesssary on **each bach**. This is what we call dynamic padding.\n",
    "\n",
    "This will speed up training in: \n",
    "- CPUs\n",
    "- GPUs\n",
    "\n",
    "Avoid doing it in: \n",
    "- TPUs (TPUs prefer fixed shapes, even when that requires extra padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### How to do it in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We have to define a `collate` function that will apply the correct amount of padding we want to batch.\n",
    "\n",
    "The ðŸ¤— Transformers library provides us with such a function via DataCollatorWithPadding.\n",
    "\n",
    "It takes a `tokenizer` when you instantiate it. This allows the collate function to: \n",
    "- Know which padding token to use\n",
    "- Know if the model needs left or right padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We test by taking a few samples.\n",
    "\n",
    "With this code we check the lengths of each entry in the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 59, 47, 67, 59, 50, 62, 32]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = tokenized_datasets[\"train\"][:8]\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
    "[len(x) for x in samples[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "With dynamic padding all the samples within the batch will be padded to length 67, the maximum in the batch.\n",
    "\n",
    "Without dynamic padding, all the samples would have to be padded to the maximum length in the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 67]),\n",
       " 'token_type_ids': torch.Size([8, 67]),\n",
       " 'attention_mask': torch.Size([8, 67]),\n",
       " 'labels': torch.Size([8])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator(samples)\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Replicate the preprocessing on the GLUE SST-2 dataset. \n",
    "Itâ€™s a little bit different since itâ€™s composed of single sentences instead of pairs, but the rest of what we did should look the same. \n",
    "\n",
    "For a harder challenge, try to write a preprocessing function that works on any of the GLUE tasks.\n",
    "\n",
    "When we refer to **GLUE task** we talk about the different tasks and datasets within the **GLUE (General Language Understanding Evaluation) benchmark**\n",
    "\n",
    "We have the following tasks in the GLUE benchmark:\n",
    "\n",
    "- **SST-2 (Stanford Sentiment Treebank)**: Binary sentiment classification task where the goal is to predict whether a sentence expresses a positive or negative sentiment.\n",
    "- **CoLA (Corpus of Linguistic Acceptability)**: A binary classification task to determine if a sentence is grammatically correct or not.\n",
    "- **MRPC (Microsoft Research Paraphrase Corpus)**: Sentence pair classification task for determining if two sentences are paraphrases of each other.\n",
    "- **QQP (Quora Question Pairs)**: Another paraphrase identification task that involves determining if two questions are semantically equivalent.\n",
    "- **STS-B (Semantic Textual Similarity Benchmark)**: A regression task focusing on predicting the similarity score between sentence pairs.\n",
    "- **MNLI (Multi-Genre Natural Language Inference)**: In this task, models are evaluated on their ability to perform three-way classification for sentence pairs: entailment, contradiction, or neutral.\n",
    "- **QNLI (Question Natural Language Inference)**: Similar to MNLI but adapted from the Stanford Question Answering Dataset (SQuAD), where the goal is to determine if a sentence answers a given question.\n",
    "- **RTE (Recognizing Textual Entailment)**: Determining whether one sentence entails another or not.\n",
    "- **WNLI (Winograd Schema Challenge)**: A coreference resolution task focusing on resolving pronouns in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset(dataset, keys_to_keep): \n",
    "    return {key:value for key, value in dataset.items() if key in keys_to_keep}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_splitter(list, size): \n",
    "    for i in range(0, len(list), size):\n",
    "        yield list[i: i + size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(row, column_names):\n",
    "    return tokenizer(row[column_names[0]], row[column_names[1]], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter GLUE dataset\n",
      " ax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 1104\n",
      "    })\n",
      "})\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 78]), 'token_type_ids': torch.Size([8, 78]), 'attention_mask': torch.Size([8, 78]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 86]), 'token_type_ids': torch.Size([8, 86]), 'attention_mask': torch.Size([8, 86]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 45]), 'token_type_ids': torch.Size([8, 45]), 'attention_mask': torch.Size([8, 45]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 69]), 'token_type_ids': torch.Size([8, 69]), 'attention_mask': torch.Size([8, 69]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 70]), 'token_type_ids': torch.Size([8, 70]), 'attention_mask': torch.Size([8, 70]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 51]), 'token_type_ids': torch.Size([8, 51]), 'attention_mask': torch.Size([8, 51]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 55]), 'token_type_ids': torch.Size([8, 55]), 'attention_mask': torch.Size([8, 55]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 40]), 'token_type_ids': torch.Size([8, 40]), 'attention_mask': torch.Size([8, 40]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 55]), 'token_type_ids': torch.Size([8, 55]), 'attention_mask': torch.Size([8, 55]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 58]), 'token_type_ids': torch.Size([8, 58]), 'attention_mask': torch.Size([8, 58]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 95]), 'token_type_ids': torch.Size([8, 95]), 'attention_mask': torch.Size([8, 95]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 35]), 'token_type_ids': torch.Size([8, 35]), 'attention_mask': torch.Size([8, 35]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 42]), 'token_type_ids': torch.Size([8, 42]), 'attention_mask': torch.Size([8, 42]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 46]), 'token_type_ids': torch.Size([8, 46]), 'attention_mask': torch.Size([8, 46]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 31]), 'token_type_ids': torch.Size([8, 31]), 'attention_mask': torch.Size([8, 31]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 47]), 'token_type_ids': torch.Size([8, 47]), 'attention_mask': torch.Size([8, 47]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 38]), 'token_type_ids': torch.Size([8, 38]), 'attention_mask': torch.Size([8, 38]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 38]), 'token_type_ids': torch.Size([8, 38]), 'attention_mask': torch.Size([8, 38]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 57]), 'token_type_ids': torch.Size([8, 57]), 'attention_mask': torch.Size([8, 57]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 43]), 'token_type_ids': torch.Size([8, 43]), 'attention_mask': torch.Size([8, 43]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 77]), 'token_type_ids': torch.Size([8, 77]), 'attention_mask': torch.Size([8, 77]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 78]), 'token_type_ids': torch.Size([8, 78]), 'attention_mask': torch.Size([8, 78]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 53]), 'token_type_ids': torch.Size([8, 53]), 'attention_mask': torch.Size([8, 53]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 55]), 'token_type_ids': torch.Size([8, 55]), 'attention_mask': torch.Size([8, 55]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 49]), 'token_type_ids': torch.Size([8, 49]), 'attention_mask': torch.Size([8, 49]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 31]), 'token_type_ids': torch.Size([8, 31]), 'attention_mask': torch.Size([8, 31]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 63]), 'token_type_ids': torch.Size([8, 63]), 'attention_mask': torch.Size([8, 63]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 53]), 'token_type_ids': torch.Size([8, 53]), 'attention_mask': torch.Size([8, 53]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 80]), 'token_type_ids': torch.Size([8, 80]), 'attention_mask': torch.Size([8, 80]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 15]), 'token_type_ids': torch.Size([8, 15]), 'attention_mask': torch.Size([8, 15]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 26]), 'token_type_ids': torch.Size([8, 26]), 'attention_mask': torch.Size([8, 26]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 29]), 'token_type_ids': torch.Size([8, 29]), 'attention_mask': torch.Size([8, 29]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 49]), 'token_type_ids': torch.Size([8, 49]), 'attention_mask': torch.Size([8, 49]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 35]), 'token_type_ids': torch.Size([8, 35]), 'attention_mask': torch.Size([8, 35]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 46]), 'token_type_ids': torch.Size([8, 46]), 'attention_mask': torch.Size([8, 46]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 49]), 'token_type_ids': torch.Size([8, 49]), 'attention_mask': torch.Size([8, 49]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 39]), 'token_type_ids': torch.Size([8, 39]), 'attention_mask': torch.Size([8, 39]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 34]), 'token_type_ids': torch.Size([8, 34]), 'attention_mask': torch.Size([8, 34]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 37]), 'token_type_ids': torch.Size([8, 37]), 'attention_mask': torch.Size([8, 37]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 37]), 'token_type_ids': torch.Size([8, 37]), 'attention_mask': torch.Size([8, 37]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 77]), 'token_type_ids': torch.Size([8, 77]), 'attention_mask': torch.Size([8, 77]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 74]), 'token_type_ids': torch.Size([8, 74]), 'attention_mask': torch.Size([8, 74]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 44]), 'token_type_ids': torch.Size([8, 44]), 'attention_mask': torch.Size([8, 44]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 29]), 'token_type_ids': torch.Size([8, 29]), 'attention_mask': torch.Size([8, 29]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 29]), 'token_type_ids': torch.Size([8, 29]), 'attention_mask': torch.Size([8, 29]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 24]), 'token_type_ids': torch.Size([8, 24]), 'attention_mask': torch.Size([8, 24]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 26]), 'token_type_ids': torch.Size([8, 26]), 'attention_mask': torch.Size([8, 26]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 26]), 'token_type_ids': torch.Size([8, 26]), 'attention_mask': torch.Size([8, 26]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 36]), 'token_type_ids': torch.Size([8, 36]), 'attention_mask': torch.Size([8, 36]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 21]), 'token_type_ids': torch.Size([8, 21]), 'attention_mask': torch.Size([8, 21]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 23]), 'token_type_ids': torch.Size([8, 23]), 'attention_mask': torch.Size([8, 23]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 23]), 'token_type_ids': torch.Size([8, 23]), 'attention_mask': torch.Size([8, 23]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 22]), 'token_type_ids': torch.Size([8, 22]), 'attention_mask': torch.Size([8, 22]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 59]), 'token_type_ids': torch.Size([8, 59]), 'attention_mask': torch.Size([8, 59]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 95]), 'token_type_ids': torch.Size([8, 95]), 'attention_mask': torch.Size([8, 95]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 56]), 'token_type_ids': torch.Size([8, 56]), 'attention_mask': torch.Size([8, 56]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 83]), 'token_type_ids': torch.Size([8, 83]), 'attention_mask': torch.Size([8, 83]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 83]), 'token_type_ids': torch.Size([8, 83]), 'attention_mask': torch.Size([8, 83]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 57]), 'token_type_ids': torch.Size([8, 57]), 'attention_mask': torch.Size([8, 57]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 75]), 'token_type_ids': torch.Size([8, 75]), 'attention_mask': torch.Size([8, 75]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 84]), 'token_type_ids': torch.Size([8, 84]), 'attention_mask': torch.Size([8, 84]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 101]), 'token_type_ids': torch.Size([8, 101]), 'attention_mask': torch.Size([8, 101]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 42]), 'token_type_ids': torch.Size([8, 42]), 'attention_mask': torch.Size([8, 42]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 97]), 'token_type_ids': torch.Size([8, 97]), 'attention_mask': torch.Size([8, 97]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 88]), 'token_type_ids': torch.Size([8, 88]), 'attention_mask': torch.Size([8, 88]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 77]), 'token_type_ids': torch.Size([8, 77]), 'attention_mask': torch.Size([8, 77]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 83]), 'token_type_ids': torch.Size([8, 83]), 'attention_mask': torch.Size([8, 83]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 83]), 'token_type_ids': torch.Size([8, 83]), 'attention_mask': torch.Size([8, 83]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 66]), 'token_type_ids': torch.Size([8, 66]), 'attention_mask': torch.Size([8, 66]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 35]), 'token_type_ids': torch.Size([8, 35]), 'attention_mask': torch.Size([8, 35]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 50]), 'token_type_ids': torch.Size([8, 50]), 'attention_mask': torch.Size([8, 50]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 77]), 'token_type_ids': torch.Size([8, 77]), 'attention_mask': torch.Size([8, 77]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 70]), 'token_type_ids': torch.Size([8, 70]), 'attention_mask': torch.Size([8, 70]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 38]), 'token_type_ids': torch.Size([8, 38]), 'attention_mask': torch.Size([8, 38]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 45]), 'token_type_ids': torch.Size([8, 45]), 'attention_mask': torch.Size([8, 45]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 39]), 'token_type_ids': torch.Size([8, 39]), 'attention_mask': torch.Size([8, 39]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 59]), 'token_type_ids': torch.Size([8, 59]), 'attention_mask': torch.Size([8, 59]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 73]), 'token_type_ids': torch.Size([8, 73]), 'attention_mask': torch.Size([8, 73]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 42]), 'token_type_ids': torch.Size([8, 42]), 'attention_mask': torch.Size([8, 42]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 41]), 'token_type_ids': torch.Size([8, 41]), 'attention_mask': torch.Size([8, 41]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 45]), 'token_type_ids': torch.Size([8, 45]), 'attention_mask': torch.Size([8, 45]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 43]), 'token_type_ids': torch.Size([8, 43]), 'attention_mask': torch.Size([8, 43]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 55]), 'token_type_ids': torch.Size([8, 55]), 'attention_mask': torch.Size([8, 55]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 76]), 'token_type_ids': torch.Size([8, 76]), 'attention_mask': torch.Size([8, 76]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 27]), 'token_type_ids': torch.Size([8, 27]), 'attention_mask': torch.Size([8, 27]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 70]), 'token_type_ids': torch.Size([8, 70]), 'attention_mask': torch.Size([8, 70]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 70]), 'token_type_ids': torch.Size([8, 70]), 'attention_mask': torch.Size([8, 70]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 73]), 'token_type_ids': torch.Size([8, 73]), 'attention_mask': torch.Size([8, 73]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 69]), 'token_type_ids': torch.Size([8, 69]), 'attention_mask': torch.Size([8, 69]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 71]), 'token_type_ids': torch.Size([8, 71]), 'attention_mask': torch.Size([8, 71]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 71]), 'token_type_ids': torch.Size([8, 71]), 'attention_mask': torch.Size([8, 71]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 85]), 'token_type_ids': torch.Size([8, 85]), 'attention_mask': torch.Size([8, 85]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 79]), 'token_type_ids': torch.Size([8, 79]), 'attention_mask': torch.Size([8, 79]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 57]), 'token_type_ids': torch.Size([8, 57]), 'attention_mask': torch.Size([8, 57]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 58]), 'token_type_ids': torch.Size([8, 58]), 'attention_mask': torch.Size([8, 58]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 48]), 'token_type_ids': torch.Size([8, 48]), 'attention_mask': torch.Size([8, 48]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 103]), 'token_type_ids': torch.Size([8, 103]), 'attention_mask': torch.Size([8, 103]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 99]), 'token_type_ids': torch.Size([8, 99]), 'attention_mask': torch.Size([8, 99]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 102]), 'token_type_ids': torch.Size([8, 102]), 'attention_mask': torch.Size([8, 102]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 82]), 'token_type_ids': torch.Size([8, 82]), 'attention_mask': torch.Size([8, 82]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 85]), 'token_type_ids': torch.Size([8, 85]), 'attention_mask': torch.Size([8, 85]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 82]), 'token_type_ids': torch.Size([8, 82]), 'attention_mask': torch.Size([8, 82]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 65]), 'token_type_ids': torch.Size([8, 65]), 'attention_mask': torch.Size([8, 65]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 35]), 'token_type_ids': torch.Size([8, 35]), 'attention_mask': torch.Size([8, 35]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 76]), 'token_type_ids': torch.Size([8, 76]), 'attention_mask': torch.Size([8, 76]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 71]), 'token_type_ids': torch.Size([8, 71]), 'attention_mask': torch.Size([8, 71]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 60]), 'token_type_ids': torch.Size([8, 60]), 'attention_mask': torch.Size([8, 60]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 46]), 'token_type_ids': torch.Size([8, 46]), 'attention_mask': torch.Size([8, 46]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 43]), 'token_type_ids': torch.Size([8, 43]), 'attention_mask': torch.Size([8, 43]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 61]), 'token_type_ids': torch.Size([8, 61]), 'attention_mask': torch.Size([8, 61]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 121]), 'token_type_ids': torch.Size([8, 121]), 'attention_mask': torch.Size([8, 121]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 121]), 'token_type_ids': torch.Size([8, 121]), 'attention_mask': torch.Size([8, 121]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 69]), 'token_type_ids': torch.Size([8, 69]), 'attention_mask': torch.Size([8, 69]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 56]), 'token_type_ids': torch.Size([8, 56]), 'attention_mask': torch.Size([8, 56]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 114]), 'token_type_ids': torch.Size([8, 114]), 'attention_mask': torch.Size([8, 114]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 115]), 'token_type_ids': torch.Size([8, 115]), 'attention_mask': torch.Size([8, 115]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 71]), 'token_type_ids': torch.Size([8, 71]), 'attention_mask': torch.Size([8, 71]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 57]), 'token_type_ids': torch.Size([8, 57]), 'attention_mask': torch.Size([8, 57]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 82]), 'token_type_ids': torch.Size([8, 82]), 'attention_mask': torch.Size([8, 82]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 22]), 'token_type_ids': torch.Size([8, 22]), 'attention_mask': torch.Size([8, 22]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 19]), 'token_type_ids': torch.Size([8, 19]), 'attention_mask': torch.Size([8, 19]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 26]), 'token_type_ids': torch.Size([8, 26]), 'attention_mask': torch.Size([8, 26]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 41]), 'token_type_ids': torch.Size([8, 41]), 'attention_mask': torch.Size([8, 41]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 30]), 'token_type_ids': torch.Size([8, 30]), 'attention_mask': torch.Size([8, 30]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 16]), 'token_type_ids': torch.Size([8, 16]), 'attention_mask': torch.Size([8, 16]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 39]), 'token_type_ids': torch.Size([8, 39]), 'attention_mask': torch.Size([8, 39]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 44]), 'token_type_ids': torch.Size([8, 44]), 'attention_mask': torch.Size([8, 44]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 44]), 'token_type_ids': torch.Size([8, 44]), 'attention_mask': torch.Size([8, 44]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 33]), 'token_type_ids': torch.Size([8, 33]), 'attention_mask': torch.Size([8, 33]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 36]), 'token_type_ids': torch.Size([8, 36]), 'attention_mask': torch.Size([8, 36]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 23]), 'token_type_ids': torch.Size([8, 23]), 'attention_mask': torch.Size([8, 23]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 35]), 'token_type_ids': torch.Size([8, 35]), 'attention_mask': torch.Size([8, 35]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 44]), 'token_type_ids': torch.Size([8, 44]), 'attention_mask': torch.Size([8, 44]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 26]), 'token_type_ids': torch.Size([8, 26]), 'attention_mask': torch.Size([8, 26]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 48]), 'token_type_ids': torch.Size([8, 48]), 'attention_mask': torch.Size([8, 48]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 96]), 'token_type_ids': torch.Size([8, 96]), 'attention_mask': torch.Size([8, 96]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 96]), 'token_type_ids': torch.Size([8, 96]), 'attention_mask': torch.Size([8, 96]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n",
      "Dynamic Batch: \n",
      "\n",
      "{'input_ids': torch.Size([8, 40]), 'token_type_ids': torch.Size([8, 40]), 'attention_mask': torch.Size([8, 40]), 'labels': torch.Size([8])}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "glue_datasets =  ['ax', 'cola', 'mnli', 'mnli_matched', 'mnli_mismatched', 'mrpc', 'qnli', 'qqp', 'rte', 'sst2', 'stsb', 'wnli']\n",
    "glue_dataset = input('Enter GLUE dataset\\n')\n",
    "\n",
    "try: \n",
    "    if glue_dataset in glue_datasets:\n",
    "        # Step 1: Load dataset\n",
    "        raw_dataset = load_dataset('glue', glue_dataset)\n",
    "        # Step 2: Tokenize dataset\n",
    "        checkpoint = 'bert-base-uncased'\n",
    "        tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "        raw_tokenized_test_dataset = raw_dataset.map(\n",
    "            lambda row: tokenize_function(row, raw_dataset.column_names['test']), \n",
    "            batched = True)\n",
    "        print(raw_tokenized_test_dataset)\n",
    "        print('\\n')\n",
    "        # Step 3: Chunking the dataset, filtering content and collating\n",
    "        batch_size = 8\n",
    "        keys_to_keep = ['input_ids', 'token_type_ids', 'attention_mask', 'label']\n",
    "        data_collator = DataCollatorWithPadding(tokenizer = tokenizer)\n",
    "        for list in list_splitter(raw_tokenized_test_dataset['test'], batch_size):\n",
    "            # filtering\n",
    "            raw_training_filtered_tokenized_dataset = filter_dataset(list, keys_to_keep)\n",
    "            # dynamic padding \n",
    "            batch = data_collator(raw_training_filtered_tokenized_dataset)\n",
    "            print(\"Dynamic Batch: \\n\")\n",
    "            print({k: v.shape for k, v in batch.items()})\n",
    "            print('\\n')\n",
    "            \n",
    "        \n",
    "except ValueError:\n",
    "    print('Not a valid dataset\\n')\n",
    "    print(f'Available: {glue_datasets}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Processing the data (PyTorch)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
